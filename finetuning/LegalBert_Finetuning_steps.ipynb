{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "898c1f41",
      "metadata": {
        "id": "898c1f41"
      },
      "source": [
        "# LegalBERT Fine-Tuning for GDPR Compliance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "249d7039",
      "metadata": {
        "id": "249d7039"
      },
      "source": [
        "## Introduction and Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5178a44",
      "metadata": {
        "id": "b5178a44"
      },
      "source": [
        "This notebook fine-tunes LegalBERT on GDPR articles and compliance datasets. The goal is to enhance the model's ability to identify GDPR compliance and non-compliance in legal texts. The notebook is structured as follows:\n",
        "1. Environment Setup\n",
        "2. Data Loading and Preprocessing\n",
        "3. Model Fine-Tuning\n",
        "4. Evaluation and Results\n",
        "5. Conclusions and Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e020d17b",
      "metadata": {
        "id": "e020d17b"
      },
      "source": [
        "### Check GPU Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43b45423",
      "metadata": {
        "id": "43b45423"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea0892be",
      "metadata": {
        "id": "ea0892be"
      },
      "source": [
        "### Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "273d69f6",
      "metadata": {
        "id": "273d69f6"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34b6145d",
      "metadata": {
        "id": "34b6145d"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88952a18",
      "metadata": {
        "id": "88952a18"
      },
      "source": [
        "## Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78c3027d",
      "metadata": {
        "id": "78c3027d"
      },
      "source": [
        "This section loads the GDPR articles used for fine-tuning the LegalBERT model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4cc4d8d",
      "metadata": {
        "id": "d4cc4d8d"
      },
      "source": [
        "**load data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94780c87",
      "metadata": {
        "id": "94780c87"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f37959d",
      "metadata": {
        "id": "7f37959d"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "# Structure your GDPR articles data\n",
        "gdpr_texts = '/content/drive/MyDrive/pfa_finetuning/gdpr/articles/preprocessed/gdpr_articles_recitals_preprocessed.jsonl'\n",
        "\n",
        "# Load dataset from JSONL file\n",
        "recitals_dataset = load_dataset(\"json\", data_files=gdpr_texts, split=\"train\")\n",
        "\n",
        "\n",
        "# Tokenize with the appropriate tokenizer (LegalBERT)\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_dataset = recitals_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Data collator for MLM\n",
        "data_collator = transformers.DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15  # Standard masking rate\n",
        ")\n",
        "\n",
        "# Load base model\n",
        "model = transformers.AutoModelForMaskedLM.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = transformers.TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/pfa_finetuning/gdpr/gdpr-legalbert\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    logging_steps=50,  # Log every 50 steps\n",
        "    eval_steps=200,  # Evaluate every 200 steps\n",
        "    report_to=\"none\"  # Avoid sending logs to external tools\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "9tr0cbM-dPZx"
      },
      "id": "9tr0cbM-dPZx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the resulting model\n",
        "model.save_pretrained(\"/content/drive/MyDrive/pfa_finetuning/gdpr/gdpr-legalbert-step1-articles-recitals-dataset\")\n",
        "tokenizer.save_pretrained(\"./content/drive/MyDrive/pfa_finetuning/gdpr/gdpr-legalbert-step1-articles-recitals-dataset\")\n",
        "\n",
        "# Simple validation script\n",
        "test_text = \"According to the GDPR, data subjects have the right to [MASK] their personal data.\"\n",
        "inputs = tokenizer(test_text, return_tensors=\"pt\").to(model.device) # Move inputs to the same device as the model\n",
        "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "\n",
        "outputs = model(**inputs)\n",
        "predictions = outputs.logits\n",
        "predicted_token_id = predictions[0, mask_token_index].argmax(axis=-1)\n",
        "predicted_token = tokenizer.decode(predicted_token_id)\n",
        "print(f\"Predicted: {predicted_token}\")  # Should predict \"access\" or similar relevant term\n"
      ],
      "metadata": {
        "id": "F5AiRiF3digC"
      },
      "id": "F5AiRiF3digC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197e25f2",
      "metadata": {
        "id": "197e25f2"
      },
      "outputs": [],
      "source": [
        "# Simple validation script\n",
        "test_text = \"Article 1 from GDPR can be summarized : Individuals can [MASK] their data and data [MASK] should be [MASK].\"\n",
        "inputs = tokenizer(test_text, return_tensors=\"pt\").to(model.device) # Move inputs to the same device as the model\n",
        "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "\n",
        "outputs = model(**inputs)\n",
        "predictions = outputs.logits\n",
        "predicted_token_id = predictions[0, mask_token_index].argmax(axis=-1)\n",
        "predicted_token = tokenizer.decode(predicted_token_id)\n",
        "print(f\"Predicted: {predicted_token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7994285a",
      "metadata": {
        "id": "7994285a"
      },
      "source": [
        "# load previously tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2cac3ea",
      "metadata": {
        "id": "d2cac3ea"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/pfa_finetuning/gdpr/gdpr-legalbert-step1-articles-recitals-dataset\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77bdf1d0",
      "metadata": {
        "id": "77bdf1d0"
      },
      "outputs": [],
      "source": [
        "chapters_path = '/content/drive/MyDrive/pfa_finetuning/gdpr/articles/preprocessed/gdpr_article_chapters_content_preprocessed.jsonl'\n",
        "chapters_dataset = load_dataset(\"json\", data_files=chapters_path, split=\"train\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0fe3010",
      "metadata": {
        "id": "a0fe3010"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = chapters_dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5b12de7",
      "metadata": {
        "id": "a5b12de7"
      },
      "outputs": [],
      "source": [
        "training_args = transformers.TrainingArguments(\n",
        "    output_dir=\"./gdpr-legalbert\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=50,  # Log every 50 steps\n",
        "    eval_steps=200,  # Evaluate every 200 steps\n",
        "    prediction_loss_only=True,\n",
        "    report_to=\"none\"  # Avoid sending logs to external tools\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the resulting model\n",
        "model.save_pretrained(\"/content/drive/MyDrive/pfa_finetuning/gdpr/gdpr-legalbert-step1-articles-chapters-dataset\")\n",
        "tokenizer.save_pretrained(\"./gdpr-legalbert-step1-articles-chapters-dataset\")\n",
        "\n",
        "# Simple validation script\n",
        "test_text = \"According to the GDPR, data subjects have the right to [MASK] their personal data.\"\n",
        "inputs = tokenizer(test_text, return_tensors=\"pt\").to(model.device) # Move inputs to the same device as the model\n",
        "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "880b286f",
      "metadata": {
        "id": "880b286f"
      },
      "outputs": [],
      "source": [
        "\n",
        "outputs = model(**inputs)\n",
        "predictions = outputs.logits\n",
        "predicted_token_id = predictions[0, mask_token_index].argmax(axis=-1)\n",
        "predicted_token = tokenizer.decode(predicted_token_id)\n",
        "print(f\"Predicted: {predicted_token}\")  # Should predict \"access\" or similar relevant term\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b20512",
      "metadata": {
        "id": "17b20512"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/pfa_finetuning/gdpr/gdpr-legalbert-step1-articles-chapters-dataset\"\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c58581db",
      "metadata": {
        "id": "c58581db"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "qa_path = '/content/drive/MyDrive/pfa_finetuning/gdpr/articles/preprocessed/gdpr_articles_qa_squad_preprocessed.json'\n",
        "qa_dataset = load_dataset(\"json\", data_files=qa_path, split=\"train\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4699042",
      "metadata": {
        "id": "c4699042"
      },
      "outputs": [],
      "source": [
        "# Accessing the inner QAs within the single dataset element\n",
        "paragraphs = qa_dataset[0]['data']['paragraphs']\n",
        "\n",
        "# Print the number of QAs\n",
        "print(f\"Number of QAs: {sum(len(p['qas']) for p in paragraphs)}\")\n",
        "\n",
        "# Print the first QA pair as an example\n",
        "print(paragraphs[0]['qas'][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ba448be",
      "metadata": {
        "id": "9ba448be"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "# Load your previously fine-tuned model\n",
        "model_path = \"/content/drive/MyDrive/pfa_finetuning/gdpr/gdpr-legalbert-step1-articles-chapters-dataset\"\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Load dataset in SQuAD format\n",
        "qa_path = '/content/drive/MyDrive/pfa_finetuning/gdpr/articles/preprocessed/gdpr_articles_qa_squad_preprocessed.json'\n",
        "qa_dataset = load_dataset(\"json\", data_files=qa_path, split=\"train\")\n",
        "\n",
        "# Preprocess the dataset to flatten and format it correctly\n",
        "def preprocess_squad_dataset(examples):\n",
        "    new_examples = {\n",
        "        \"question\": [],\n",
        "        \"context\": [],\n",
        "        \"id\": [],\n",
        "        \"answers\": []\n",
        "    }\n",
        "\n",
        "    for article in examples['data']:\n",
        "        for paragraph in article['paragraphs']:\n",
        "            context = paragraph['context']\n",
        "            for qa in paragraph['qas']:\n",
        "                new_examples[\"question\"].append(qa['question'])\n",
        "                new_examples[\"context\"].append(context)\n",
        "                new_examples[\"id\"].append(qa['id'])\n",
        "                new_examples[\"answers\"].append({\n",
        "                    \"text\": [answer['text'] for answer in qa['answers']],\n",
        "                    \"answer_start\": [answer['answer_start'] for answer in qa['answers']]\n",
        "                })\n",
        "\n",
        "    return new_examples\n",
        "\n",
        "# Apply preprocessing\n",
        "flat_qa_dataset = qa_dataset.map(\n",
        "    preprocess_squad_dataset,\n",
        "    batched=True,\n",
        "    remove_columns=qa_dataset.column_names\n",
        ")\n",
        "\n",
        "# Tokenize the dataset\n",
        "def prepare_train_features(examples):\n",
        "    # Tokenize questions and contexts\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"context\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=512,\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    # Map from token spans back to character spans in the original document\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # The start_positions and end_positions represent answer span token indices\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # Match back to the original example\n",
        "        example_idx = sample_mapping[i]\n",
        "\n",
        "        # Get answer for this example\n",
        "        answers = examples[\"answers\"][example_idx]\n",
        "        answer_starts = answers[\"answer_start\"]\n",
        "        answer_texts = answers[\"text\"]\n",
        "\n",
        "        # By default, use first answer (can be modified for multiple answers)\n",
        "        if len(answer_starts) > 0:\n",
        "            answer_start_char = answer_starts[0]\n",
        "            answer_text = answer_texts[0]\n",
        "            answer_end_char = answer_start_char + len(answer_text)\n",
        "        else:\n",
        "            # No answers - use special token positions\n",
        "            tokenized_examples[\"start_positions\"].append(0)\n",
        "            tokenized_examples[\"end_positions\"].append(0)\n",
        "            continue\n",
        "\n",
        "        # Find token indices that cover the answer\n",
        "        token_start_index = 0\n",
        "        token_end_index = 0\n",
        "\n",
        "        # Find the start token index\n",
        "        for idx, (start, end) in enumerate(offsets):\n",
        "            if start <= answer_start_char < end:\n",
        "                token_start_index = idx\n",
        "                break\n",
        "\n",
        "        # Find the end token index\n",
        "        for idx, (start, end) in enumerate(offsets):\n",
        "            if start < answer_end_char <= end:\n",
        "                token_end_index = idx\n",
        "                break\n",
        "\n",
        "        # Add start and end positions\n",
        "        tokenized_examples[\"start_positions\"].append(token_start_index)\n",
        "        tokenized_examples[\"end_positions\"].append(token_end_index)\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "# Apply feature preparation\n",
        "tokenized_qa = flat_qa_dataset.map(\n",
        "    prepare_train_features,\n",
        "    batched=True,\n",
        "    remove_columns=flat_qa_dataset.column_names\n",
        ")\n",
        "\n",
        "# Split the data\n",
        "split_dataset = tokenized_qa.train_test_split(test_size=0.1)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "eval_dataset = split_dataset[\"test\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14763e9b",
      "metadata": {
        "id": "14763e9b"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/pfa_finetuning/gdpr/gdpr-legalbert-qa\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100\n",
        "  )\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "qAspw7AkeCHd"
      },
      "id": "qAspw7AkeCHd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save_pretrained(\"/content/drive/MyDrive/pfa_finetuning/gdpr/gdpr-legalbert-qa-final\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/pfa_finetuning/gdpr/gdpr-legalbert-qa-final\")\n",
        "\n"
      ],
      "metadata": {
        "id": "QsWEDe3deEO1"
      },
      "id": "QsWEDe3deEO1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function for QA\n",
        "def compute_metrics(eval_pred):\n",
        "    metric = evaluate.load(\"squad\")\n",
        "\n",
        "    predictions, labels = eval_pred\n",
        "    # Extract start and end logits\n",
        "    start_logits, end_logits = predictions\n",
        "\n",
        "    # Format predictions for evaluation\n",
        "    formatted_predictions = [\n",
        "        {\"id\": eval_dataset[\"id\"][i], \"prediction_text\": \"dummy\", \"no_answer_probability\": 0.0}\n",
        "        for i in range(len(start_logits))\n",
        "    ]\n",
        "\n",
        "    # Format references for evaluation\n",
        "    references = [\n",
        "        {\"id\": eval_dataset[\"id\"][i], \"answers\": {\"text\": [eval_dataset[\"answers\"][i][\"text\"]], \"answer_start\": [eval_dataset[\"answers\"][i][\"answer_start\"]]}}\n",
        "        for i in range(len(labels))\n",
        "    ]\n",
        "\n",
        "    # Compute metrics\n",
        "    result = metric.compute(predictions=formatted_predictions, references=references)\n",
        "    return result\n",
        "\n",
        "# Evaluate\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation results: {eval_results}\")"
      ],
      "metadata": {
        "id": "iYVHYkZPeG-b"
      },
      "id": "iYVHYkZPeG-b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "615dce0f",
      "metadata": {
        "id": "615dce0f"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the fine-tuned model\n",
        "qa_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"/content/drive/MyDrive/pfa_finetuning/gdpr/gdpr-legalbert-qa-final\",\n",
        "    tokenizer=\"/content/drive/MyDrive/pfa_finetuning/gdpr/gdpr-legalbert-qa-final\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34352bb7",
      "metadata": {
        "id": "34352bb7"
      },
      "outputs": [],
      "source": [
        "# Test with examples\n",
        "test_examples = [\n",
        "    {\n",
        "        \"question\": \"What is the purpose of GDPR?\",\n",
        "        \"context\": \"The GDPR aims to strengthen the protection of personal data within the EU. It lays down rules relating to the protection of natural persons with regard to the processing of personal data and rules relating to the free movement of personal data.\"\n",
        "    },\n",
        "    # Add more test examples\n",
        "]\n",
        "\n",
        "for example in test_examples:\n",
        "    result = qa_pipeline(example)\n",
        "    print(f\"Question: {example['question']}\")\n",
        "    print(f\"Answer: {result['answer']}\")\n",
        "    print(f\"Score: {result['score']:.4f}\")\n",
        "    print(\"-\" * 50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}